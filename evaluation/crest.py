import json
import re
import sys
from collections import Counter
from typing import Iterable, List, Literal, Tuple

from evaluation.method.baseline import BaselineMethod

from .prompts.crest import (
    NON_REFUSAL_EVALUATION_PROMPT,
    DIRECT_ANSWER_GENERATION_PROMPT,
    NAIVE_COT_ANSWER_GENERATION_PROMPT,
    COD_ANSWER_GENERATION_PROMPT,
    SEMI_STRUCTURED_ANSWER_GENERATION_PROMPT,
)
from .types import ERROR_TYPES, QAExampleAnswered, QAExampleEvaluated, QAFinalDatum
from .utils import openai_chat_completion, remove_citation_from_answer

from datasets import disable_progress_bar

disable_progress_bar()


def predict(
    example: QAFinalDatum, method: BaselineMethod
) -> Iterable[QAExampleAnswered]:
    parsed_answer, prompt, entire_answer, prompt_tokens, completion_tokens = (
        method.predict(example["query"], json.loads(example["documents"]))
    )

    example["raw_predicted_answer"] = entire_answer
    example["predicted_answer"] = parsed_answer
    example["prediction_details"] = {
        "prompt": prompt,
        "method": method.__class__.__name__,
        "attrs": vars(method),
    }
    example["predict_usage"] = {
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
    }
    return example


def parse_non_refusal_evaluation(output: str) -> Tuple[float, str]:
    """Parse the output of a non-refusal evaluation.

    Args:
        output (str): The output generated by OpenAI Chat to evaluate the example.
    Returns:
        The score of the evaluation.
    """
    score = None
    if found := re.findall(
        r"\*\*Decision\*\*:[\"'\s*]*(Correct|Partially Correct|Wrong)", output
    ):
        try:
            match found[0]:
                case "Correct":
                    score = 2
                case "Partially Correct":
                    score = 1
                case "Wrong":
                    score = 0
        except ValueError:
            return score, ""

    if found := re.findall(rf"\*\*ErrorType\*\*:\s*({'|'.join(ERROR_TYPES)})", output):
        error_type = found[0]
    else:
        error_type = ""

    return score, error_type


def non_refusal_evaluation(
    example: QAExampleAnswered, model_name: str, seed: int
) -> QAExampleEvaluated:
    """Evaluate a non-refusal example using OpenAI Chat.

    Args:
        example (dict): The example to evaluate.
        model_name (str): The name of the model to use.
        seed (int): The seed to use for the evaluation.
    Returns:
        The evaluated example.
    """
    example["evaluation_result"] = {
        "meta": {"model": model_name, "seed": seed, "error": None},
        "score": None,
        "justification": None,
        "error_type": None,
        "usage": {},
    }

    if len(example["predicted_answer"]) == 0:
        example["evaluation_result"]["score"] = 0.0
        return example

    prompt = NON_REFUSAL_EVALUATION_PROMPT.format(
        question=example["query"],
        golden_answer=example["answer"],
        predicted_answer=remove_citation_from_answer(example["predicted_answer"]),
    )

    output, usage, error = openai_chat_completion(
        model_name=model_name,
        messages=[
            {"role": "user", "content": prompt},
        ],
        seed=seed,
    )

    if error is not None:
        example["evaluation_result"]["meta"]["error"] = str(error)
        return example

    score, error_type = parse_non_refusal_evaluation(output)
    if score is None:
        print(f"[!] Parsing Error: {output}", file=sys.stderr)

    example["evaluation_result"]["score"] = score
    example["evaluation_result"]["error_type"] = error_type
    example["evaluation_result"]["justification"] = output
    example["evaluation_result"]["usage"] = {
        "prompt_tokens": usage.prompt_tokens,
        "completion_tokens": usage.completion_tokens,
    }
    return example


def refusal_evaluation(example: QAExampleAnswered) -> QAExampleEvaluated:
    """Evaluate a refusal example using OpenAI Chat.

    Args:
        example (dict): The example to evaluate.
        model_name (str): The name of the model to use.
        seed (int): The seed to use for the evaluation.
    Returns:
        The evaluated example.
    """
    correctness = float(
        "unanswerable" in example["predicted_answer"].lower()
    )  # Correct or Wrong
    example["evaluation_result"] = {
        "meta": {"model": None, "seed": None, "error": None},
        "score": correctness,
        "usage": {},
    }
    return example


def citation_evaluation(example: QAExampleAnswered) -> QAExampleEvaluated:
    """Evaluate a citation ability of the model.

    Args:
        example (dict): The example to evaluate.
        model_name (str): The name of the model to use.
        seed (int): The seed to use for the evaluation.
    Returns:
        The evaluated example.
    """
    document_number_dict = {
        doc["docid"]: str(idx + 1) for idx, doc in enumerate(json.loads(example["documents"]))
    }
    golden_docs = [document_number_dict[i] for i in json.loads(example["citation_ids"])]
    citation_pattern = re.compile(r"\[(\d+)\]", re.IGNORECASE)
    predicted_docs = list(set(citation_pattern.findall(example["predicted_answer"])))

    precision = (
        len(set(predicted_docs) & set(golden_docs)) / len(predicted_docs)
        if len(predicted_docs) != 0
        else 0.0
    )
    recall = (
        len(set(predicted_docs) & set(golden_docs)) / len(golden_docs)
        if len(golden_docs) != 0
        else 0.0
    )

    example["evaluation_result"]["citation_precision"] = precision
    example["evaluation_result"]["citation_recall"] = recall
    return example


def aggregate_score(
    dataset: Iterable[QAExampleEvaluated],
    language: Literal["en", "ko"],
    difficulty_type: List[Literal["SimpleQA", "ComplexQA"]] = [
        "SimpleQA",
        "ComplexQA",
    ],
) -> Tuple[float, float, float, int]:
    """Aggregate the scores of a dataset.

    Args:
        dataset (list): The dataset to aggregate the scores
    Returns:
        The average score and the number of invalid examples
    """
    dataset = dataset.filter(
        lambda x: json.loads(x["meta"])["language"] == language
        and json.loads(x["meta"])["difficulty_type"] in difficulty_type
    )

    valid_scores = [
        example["evaluation_result"]["score"]
        for example in dataset
        if example["evaluation_result"]["score"] is not None
    ]
    num_invalid = len(dataset) - len(valid_scores)

    if not valid_scores:
        return 0.0, 0.0, 0.0, num_invalid

    counter = Counter(valid_scores)
    num_correct = counter.get(2, 0)
    num_partially_correct = counter.get(1, 0)
    num_wrong = counter.get(0, 0)

    if len(valid_scores) == 0:
        print("[!] No valid scores found in the dataset.", file=sys.stderr)
        return 0.0, 0.0, 0.0, num_invalid

    correct_rate = num_correct / len(valid_scores)
    partially_correct_rate = num_partially_correct / len(valid_scores)
    wrong_rate = num_wrong / len(valid_scores)

    return correct_rate, partially_correct_rate, wrong_rate, num_invalid


def aggregate_citation_score(
    dataset: Iterable[QAExampleEvaluated],
    language: Literal["en", "ko"],
    difficulty_type: Literal["SimpleQA", "ComplexQA"],
) -> Tuple[float, float]:
    dataset = dataset.filter(
        lambda x: json.loads(x["meta"])["language"] == language
        and json.loads(x["meta"])["difficulty_type"] == difficulty_type
    )
    precision = [
        example["evaluation_result"]["citation_precision"] for example in dataset
    ]
    recall = [example["evaluation_result"]["citation_recall"] for example in dataset]
    return (
        sum(precision) / len(precision) if precision else 0.0,
        sum(recall) / len(recall) if recall else 0.0,
    )


def calculate_unified_score(
    dataset: Iterable[QAExampleEvaluated],
    language: Literal["en", "ko"],
    difficulty_type: List[Literal["SimpleQA", "ComplexQA"]],
) -> float:
    refusal_score, refusal_cnt = 0.0, 0
    for instance in dataset["refusal"].filter(
        lambda x: json.loads(x["meta"])["language"] == language
        and json.loads(x["meta"])["difficulty_type"] in difficulty_type
    ):
        refusal_score += instance["evaluation_result"]["score"] or 0.0
        refusal_cnt += 1
    refusal_avg_score = refusal_score / refusal_cnt if refusal_cnt != 0 else 0.0
    non_refusal_score, non_refusal_cnt = 0.0, 0
    for instance in dataset["non_refusal"].filter(
        lambda x: json.loads(x["meta"])["language"] == language
        and json.loads(x["meta"])["difficulty_type"] in difficulty_type
    ):
        if instance["evaluation_result"]["score"] == 0:
            is_refusal = bool("unanswerable" in instance["predicted_answer"].lower())
            if is_refusal:
                non_refusal_score += -1.0
            else:
                non_refusal_score += 0.0
        else:
            non_refusal_score += (instance["evaluation_result"]["score"] or 0.0) / 2
        non_refusal_cnt += 1
    non_refusal_avg_score = (
        non_refusal_score / non_refusal_cnt if non_refusal_cnt != 0 else 0.0
    )
    return (refusal_avg_score + non_refusal_avg_score) / 2


def calculate_reasoning_type_unified_score(
    dataset: Iterable[QAExampleEvaluated],
) -> tuple[dict, dict]:
    reasoning_type_refusal_score_dict = {
        "multi-constraint reasoning": [],
        "numerical reasoning": [],
        "temporal reasoning": [],
        "tabular reasoning": [],
        "format reasoning": [],
        "textual reasoning": [],
    }
    reasoning_type_non_refusal_score_dict = {
        "multi-constraint reasoning": [],
        "numerical reasoning": [],
        "temporal reasoning": [],
        "tabular reasoning": [],
        "format reasoning": [],
        "textual reasoning": [],
    }

    for instance in dataset["refusal"]:
        for reasoning_types in instance["reasoning_type"]:
            for reasoning_type in reasoning_types:
                if reasoning_type not in reasoning_type_refusal_score_dict:
                    continue
                reasoning_type_refusal_score_dict[reasoning_type].append(
                    instance["evaluation_result"]["score"]
                )

    for instance in dataset["non_refusal"]:
        for reasoning_types in instance["reasoning_type"]:
            for reasoning_type in reasoning_types:
                if reasoning_type not in reasoning_type_non_refusal_score_dict:
                    continue
                if instance["evaluation_result"]["score"] == 0:
                    is_refusal = bool(
                        "unanswerable" in instance["predicted_answer"].lower()
                    )
                    if is_refusal:
                        reasoning_type_non_refusal_score_dict[reasoning_type].append(
                            -1.0
                        )
                    else:
                        reasoning_type_non_refusal_score_dict[reasoning_type].append(
                            0.0
                        )
                else:
                    reasoning_type_non_refusal_score_dict[reasoning_type].append(
                        (instance["evaluation_result"]["score"]) / 2
                    )

    reasoning_type_refusal_avg_score_dict = {
        k: sum(v) / len(v) if len(v) != 0 else 0.0
        for k, v in reasoning_type_refusal_score_dict.items()
    }
    reasoning_type_non_refusal_avg_score_dict = {
        k: sum(v) / len(v) if len(v) != 0 else 0.0
        for k, v in reasoning_type_non_refusal_score_dict.items()
    }

    reasoning_type_unified_score_dict = {
        k: (
            reasoning_type_refusal_avg_score_dict[k]
            + reasoning_type_non_refusal_avg_score_dict[k]
        )
        / 2
        for k in reasoning_type_refusal_avg_score_dict
    }
    return reasoning_type_unified_score_dict
